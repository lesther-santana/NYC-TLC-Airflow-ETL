import pendulum
import os

from airflow.decorators import dag, task
from airflow.models import Variable
from airflow.providers.amazon.aws.hooks.s3 import S3Hook
from airflow.providers.amazon.aws.operators.s3 import S3ListOperator



doc_md_DAG = """
### TLC ETL Dag

This DAG extracts NYC TLC data high volume FHV trips from an AWS S3 bucket.

The dag needs the following Airflow Variables:

- HV_FHV_TABLE_ID.
"""



@dag(
    schedule=None,
    start_date=pendulum.now('UTC'),
    catchup=False,
    tags=["example"],
    doc_md=doc_md_DAG
)
def load_to_bigquery():
    """
    Get TLC parquet files from aws S3 bucket, transform and push them to Bigquery 
    """

    data_path = os.path.join(os.path.dirname(__file__),"data/")
    transformed_data_path = os.path.join(os.path.dirname(__file__),"cleaned_data/")

    def clean_folder(folder_path):
        """
        Remove the contents of the folder.

        Args:
            folder_path (str): Path to the folder.

        Returns:
            None
        """
        import shutil
        for filename in os.listdir(folder_path):
            file_path = os.path.join(folder_path, filename)
            if os.path.isfile(file_path):
                os.unlink(file_path)
            elif os.path.isdir(file_path):
                shutil.rmtree(file_path)
    
    def find_json_file(directory):
        """
        Find the first JSON file in the directory and its subdirectories.

        Args:
            directory (str): Path to the directory.
        Returns:
            str: Path to the JSON file.
        """
        json_files = []
        for root, dirs, files in os.walk(directory):
            for file in files:
                if file.endswith(".json"):
                    json_files.append(os.path.join(root, file))
        return json_files[0]

    @task(task_id="extract")
    def extract(s3_paths: list):
        """
        Download parquet files to the data directory.

        Args:
            s3_paths (list): List of S3 file paths.

        Returns:
            None
        """
        os.makedirs(os.path.dirname(data_path), exist_ok=True)
        clean_folder(data_path)
        hook = S3Hook("s3_conn")
        i = 0
        for s3_path in s3_paths:
            #exclude "trip data/"
            if len(s3_path) <=10:
                continue
            parts = s3_path.split("/")[-1].split('_')
            year = int(parts[2][:4])
            type_trip = parts[0]
            if s3_path.endswith(".parquet") and year >= 2022 and type_trip == "fhvhv":
                hook.download_file(
                    bucket_name ="nyc-tlc",
                    key=s3_path,
                    preserve_file_name=True,
                    local_path=data_path,
                    use_autogenerated_subdir=False
                )
                i += 1
        print("Downloaded {} files".format(i))

    @task()
    def transform():
        """
        Drop unnecessary columns and enforce schema for BigQuery.

        Returns:
            None
        """
        
        os.makedirs(os.path.dirname(transformed_data_path), exist_ok=True)
        clean_folder(transformed_data_path)
        
        import pandas as pd

        cols_to_exclude = ["access_a_ride_flag", "wav_request_flag", "wav_match_flag", "originating_base_num", "on_scene_datetime"]

        for filename in os.listdir(data_path):
            file_path = os.path.join(data_path, filename)
            new_path = os.path.join(transformed_data_path,f"tr_{filename}")
            columns_to_read = [
                "hvfhs_license_num", 
                "request_datetime",
                "pickup_datetime",
                "dropoff_datetime",
                "PULocationID",
                "DOLocationID",
                "trip_miles",
                "trip_time",
                "base_passenger_fare",
                "tolls",
                "airport_fee",
                "tips",
                "driver_pay"
            ]
            data = pd.read_parquet(file_path, columns=columns_to_read)
            data["trip_time"] = data["trip_time"].astype("float32")
            data.to_parquet(new_path)
            print(new_path)

    @task()
    def load():
        """
        Push dataframe to Google BigQuery Table
        
        Returns:
            None
        """
        from google.cloud import bigquery

        os.environ["GOOGLE_APPLICATION_CREDENTIALS"] = find_json_file(os.path.dirname(__file__))
        schema = schema = [
            bigquery.SchemaField("hvfhs_license_num", "STRING"),
            bigquery.SchemaField("request_datetime", "TIMESTAMP"),
            bigquery.SchemaField("pickup_datetime", "TIMESTAMP"),
            bigquery.SchemaField("dropoff_datetime", "TIMESTAMP"),
            bigquery.SchemaField("PUlocationID", "INTEGER"),
            bigquery.SchemaField("DOlocationID", "INTEGER"),
            bigquery.SchemaField("trip_miles", "FLOAT"),
            bigquery.SchemaField("trip_time", "FLOAT"),
            bigquery.SchemaField("base_passenger_fare", "FLOAT"),
            bigquery.SchemaField("tolls", "FLOAT"),
            bigquery.SchemaField("airport_fee", "FLOAT"),
            bigquery.SchemaField("tips", "FLOAT"),
            bigquery.SchemaField("driver_pay", "FLOAT")
        ]

        table_id = Variable.get("HV_FHV_TABLE_ID")
        job_config = bigquery.LoadJobConfig(
            source_format=bigquery.SourceFormat.PARQUET,
            schema=schema
        )
        client = bigquery.Client()
        for filename in os.listdir(transformed_data_path):
            file_path = os.path.join(transformed_data_path,filename)
            with open(file_path, "rb", buffering=0) as source_file:
                job = client.load_table_from_file(source_file, table_id, job_config=job_config)
                job.result()  # Waits for the job to complete.
                print(file_path)
        print("Done pushing to Bigquery")

    s3_files = S3ListOperator(
            task_id='list_3s_files',
            aws_conn_id="s3_conn",
            bucket="nyc-tlc",
            prefix="trip data/",
            delimiter="/"
    )

    extract_task = extract(s3_files.output)
    transform_task = transform() 
    extract_task >> transform_task >> load()

dag = load_to_bigquery()